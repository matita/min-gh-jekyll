---
title: "Google Unveils Neural Network with “Superhuman” Ability to Determine the Location of Almost Any Image"
description: "Guessing the location of a randomly chosen Street View image is hard, even for well-traveled humans. But Google’s latest artificial-intelligence machine manages it with relative ease."
link: "https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/"
saved: "2016-02-26 14:43:45"
---

	<div class="article-body hang-punctiation">

		
			


<div class="l-article-topper l-article-topper--standard" data-topper-layout="standard" data-topper-type="standard">

	

	<div class="article-topper__hgroup">

		

			
			
			<h1 class="article-topper__topic"><a href="https://www.technologyreview.com/c/computing/" class="entry__term">Computing</a></h1>
			

			<h2 class="article-topper__title">Google Unveils Neural Network with “Superhuman” Ability to Determine the Location of Almost Any Image</h2>

		

			
			<p class="article-topper__subtitle">Guessing the location of a randomly chosen Street View image is hard, even for well-traveled humans. But Google’s latest artificial-intelligence machine manages it with relative ease.</p>
			

			<ul class="article-topper__meta-info">
				
				<li class="article-topper__meta-item">by 
	<a href="https://www.technologyreview.com/profile/emerging-technology-from-the-arxiv/" class="">Emerging Technology from the arXiv</a>
</li>
				

				
				<li class="article-topper__meta-item">February 24, 2016</li>
				
			</ul>

		

	</div><!-- // end .article-topper__hgroup -->

	

		

		

		<div rv-show="entrySet.sponsor" class="cta--topper">
			<div class="sponsor">
				<p class="sponsor__leadin">
					<span rv-html="entrySet.sponsor.sponsored_by_text"></span>
					<span rv-hide="entrySet.sponsor.sponsored_by_text">Sponsored by</span>
				</p>
				<a rv-show="entrySet.sponsor.url" rv-href="entrySet.sponsor.url" class="sponsor__logo">
					<img rv-show="entrySet.sponsor.logo" rv-src="entrySet.sponsor.logo.url" rv-alt="entrySet.sponsor.name" src="https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/?utm_source=SitePoint&amp;utm_medium=email&amp;utm_campaign=Versioning">
					<span rv-hide="entrySet.sponsor.logo" rv-html="entrySet.sponsor.name"></span>
				</a>
				<span rv-hide="entrySet.sponsor.url" class="sponsor__logo">
					<img rv-show="entrySet.sponsor.logo" rv-src="entrySet.sponsor.logo.url" rv-alt="entrySet.sponsor.name" src="https://www.technologyreview.com/s/600889/google-unveils-neural-network-with-superhuman-ability-to-determine-the-location-of-almost/?utm_source=SitePoint&amp;utm_medium=email&amp;utm_campaign=Versioning">
					<span rv-hide="entrySet.sponsor.logo" rv-html="entrySet.sponsor.name"></span>
				</span>
			</div>
		</div>

	
</div>



		

		

		
		<div class="l-article-topper__social-list">
			
		</div>
		

		
		<div class="article-body__content">
			

				

				
					<p><span class="dropcap">H</span>ere’s a tricky task. Pick a photograph from the Web at random. Now try to work out where it was taken using only the image itself. If the image shows a famous building or landmark, such as the Eiffel Tower or Niagara Falls, the task is straightforward. But the job becomes significantly harder when the image lacks specific location cues or is taken indoors or shows a pet or food or some other detail.</p>
				

			

				
					
	

	

				

				
					<p>Nevertheless, humans are surprisingly good at this task. To help, they bring to bear all kinds of knowledge about the world such as the type and language of signs on display, the types of vegetation, architectural styles, the direction of traffic, and so on. Humans spend a lifetime picking up these kinds of geolocation cues.</p>
				

			

				

				
					<p>So it’s easy to think that machines would struggle with this task. And indeed, they have.</p>
				

			

				

				
					<p>Today, that changes thanks to the work of Tobias Weyand, a computer vision specialist at Google, and a couple of pals. These guys have trained a deep-learning machine to work out the location of almost any photo using only the pixels it contains.</p>
				

			

				

				
					<p>Their new machine significantly outperforms humans and can even use a clever trick to determine the location of indoor images and pictures of specific things such as pets, food, and so on that have no location cues.</p>
				

			

				

				
					<p>Their approach is straightforward, at least in the world of machine learning. Weyand and co begin by dividing the world into a grid consisting of over 26,000 squares of varying size that depend on the number of images taken in that location.</p>
				

			

				

				
					<p>So big cities, which are the subjects of many images, have a more fine-grained grid structure than more remote regions where photographs are less common. Indeed, the Google team ignored areas like oceans and the polar regions, where few photographs have been taken.</p>
				

			

				

				
					
					


	


<figure class="l-article-img l-article-img--text-col" data-widget-type="imageset" data-widget-layout="text-col">

	
		

		
			
	<picture>
		
			
			
			
			
			

			
			
			
			

			
			<source media="(min-width: 1024px)" srcset="https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=590&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 1x,https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=1180&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 2x">
			

			
			<source media="(min-width: 850px)" srcset="https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=1024&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 1x,https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=2048&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 2x">
			

			
			<source media="(min-width: 550px)" srcset="https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=850&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 1x,https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=1700&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 2x">
			

			
			<source media="(min-width: 401px)" srcset="https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=550&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 1x,https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=1100&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 2x">
			

			
			<source media="(min-width: 0px)" srcset="https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=401&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 1x,https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=802&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 2x">
			

			<img class="article-img article-img--text-col" src="https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=590&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614" srcset="https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=590&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 1x,https://d267cvn3rvuq91.cloudfront.net/i/images/planet.jpg?sw=1180&amp;cx=0&amp;cy=0&amp;cw=928&amp;ch=614 2x">
		
	</picture>


		

		
	

	
</figure>

				

			

				

				
					<p>Next, the team created a database of geolocated images from the Web and used the location data to determine the grid square in which each image was taken. This data set is huge, consisting of 126 million images along with their accompanying Exif location data.</p>
				

			

				

				
					<p>Weyand and co used 91 million of these images to teach a powerful neural network to work out the grid location using only the image itself. Their idea is to input an image into this neural net and get as the output a particular grid location or a set of likely candidates.</p>
				

			

				

				
					<p>They then validated the neural network using the remaining 34 million images in the data set. Finally they tested the network—which they call PlaNet—in a number of different ways to see how well it works.</p>
				

			

				

				
					<p>The results make for interesting reading. To measure the accuracy of their machine, they fed it 2.3 million geotagged images from Flickr to see whether it could correctly determine their location. “PlaNet is able to localize 3.6 percent of the images at street-level accuracy and 10.1 percent at city-level accuracy,” say Weyand and co. What’s more, the machine determines the country of origin in a further 28.4 percent of the photos and the continent in 48.0 percent of them.</p>
				

			

				

				
					<p>That’s pretty good. But to show just how good, Weyand and co put PlaNet through its paces in a test against 10 well-traveled humans. For the test, they used an online game that presents a player with a random view taken from Google Street View and asks him or her to pinpoint its location on a map of the world. &nbsp;&nbsp;</p>
				

			

				

				
					<p>Anyone can play at&nbsp;<a href="http://www.geoguessr.com/" target="_blank">www.geoguessr.com</a>. Give it a try—it’s a lot of fun and more tricky than it sounds.</p>
				

			

				

				
					<p>Needless to say, PlaNet trounced the humans. “In total, PlaNet won 28 of the 50 rounds with a median localization error of 1131.7 km, while the median human localization error was 2320.75 km,” say Weyand and co. “[This] small-scale experiment shows that PlaNet reaches superhuman performance at the task of geolocating Street View scenes.”</p>
				

			

				

				
					<p>An interesting question is how PlaNet performs so well without being able to use the cues that humans rely on, such as vegetation, architectural style, and so on. But Weyand and co say they know why: "We think PlaNet has an advantage over humans because it has seen many more places than any human can ever visit and has learned subtle cues of different scenes that are even hard for a well-traveled human to distinguish.”</p>
				

			

				

				
					<p>They go further and use the machine to locate images that do not have location cues, such as those taken indoors or of specific items. This is possible when images are part of albums that have all been taken at the same place. The machine simply looks through other images in the album to work out where they were taken and assumes the more specific image was taken in the same place.</p>
				

			

				

				
					<p>That’s impressive work that shows deep neural nets flexing their muscles once again. Perhaps more impressive still is that the model uses a relatively small amount of memory unlike other approaches that use gigabytes of the stuff.&nbsp; “Our model uses only 377 MB, which even fits into the memory of a smartphone,” say Weyand and co.</p>
				

			

				

				
					<p>That’s a tantalizing idea—the power of a superhuman neural network on a smartphone. It surely won’t be long now!</p>
				

			

				

				
					<p>Ref:&nbsp;<a href="http://arxiv.org/abs/1602.05314" target="_blank">arxiv.org/abs/1602.05314</a>&nbsp;: PlaNet—Photo Geolocation with Convolutional Neural Networks</p>
				

			

			
		</div>
		

		


	</div>

	
	

	

